{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology Alignment with on2vec\n",
    "\n",
    "This notebook demonstrates automated ontology alignment using on2vec embeddings. We'll show how to:\n",
    "\n",
    "1. Find equivalent concepts between different ontologies\n",
    "2. Detect subsumption relationships across ontologies\n",
    "3. Build alignment mappings automatically\n",
    "4. Validate and score alignment quality\n",
    "5. Create cross-ontology concept bridges\n",
    "6. Visualize alignment networks\n",
    "\n",
    "## Use Case: Biomedical Ontology Integration\n",
    "Different research communities often develop their own ontologies for the same domain. For example, disease concepts appear in multiple ontologies (MONDO, DO, ICD, SNOMED). Automated alignment helps integrate these knowledge resources and enables cross-ontology queries and reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import umap\n",
    "\n",
    "# on2vec imports\n",
    "from on2vec import (\n",
    "    load_embeddings_as_dataframe,\n",
    "    train_ontology_embeddings,\n",
    "    embed_ontology_with_model,\n",
    "    build_graph_from_owl\n",
    ")\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Ontologies for Alignment\n",
    "\n",
    "We'll work with multiple ontologies and create embeddings optimized for alignment tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define ontologies for alignment\n",
    "alignment_ontologies = {\n",
    "    'source': {\n",
    "        'name': 'edam',\n",
    "        'file': 'EDAM.owl',\n",
    "        'domain': 'bioinformatics'\n",
    "    },\n",
    "    'target': {\n",
    "        'name': 'cvdo', \n",
    "        'file': 'cvdo.owl',\n",
    "        'domain': 'cardiovascular'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Additional ontologies for multi-way alignment (if available)\n",
    "additional_ontologies = {\n",
    "    'cl': 'owl_files/fao.owl',  # Cell ontology\n",
    "    'go': 'go.owl',  # Gene ontology (if available)\n",
    "}\n",
    "\n",
    "def prepare_alignment_embeddings(ontologies, use_text=True, alignment_optimized=True):\n",
    "    \"\"\"Generate embeddings optimized for alignment tasks.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for role, ont_info in ontologies.items():\n",
    "        name = ont_info['name']\n",
    "        owl_file = ont_info['file']\n",
    "        \n",
    "        if not os.path.exists(owl_file):\n",
    "            print(f\"‚ö†Ô∏è Skipping {name}: {owl_file} not found\")\n",
    "            continue\n",
    "            \n",
    "        model_file = f\"{name}_alignment_model.pt\"\n",
    "        embedding_file = f\"{name}_alignment_embeddings.parquet\"\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing {name} ({role}) for alignment...\")\n",
    "        \n",
    "        # Train alignment-optimized model\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"  Training alignment model for {name}...\")\n",
    "            \n",
    "            # Use settings optimized for alignment\n",
    "            train_result = train_ontology_embeddings(\n",
    "                owl_file=owl_file,\n",
    "                model_output=model_file,\n",
    "                model_type=\"gcn\",\n",
    "                hidden_dim=256,  # Larger for better representation\n",
    "                out_dim=128,     # Higher dimensional embeddings\n",
    "                epochs=100,\n",
    "                loss_fn=\"cosine\",  # Cosine loss for similarity tasks\n",
    "                learning_rate=0.01\n",
    "                text_model_type=\"sentence_transformer\",\n",
    "                text_model_name=\"all-MiniLM-L6-v2\",\n",
    "                fusion_method=\"concat\"  # Rich combined representation\n",
    "            )\n",
    "        \n",
    "        # Generate embeddings\n",
    "        if not os.path.exists(embedding_file):\n",
    "            print(f\"  Generating embeddings for {name}...\")\n",
    "            embed_result = embed_ontology_with_model(\n",
    "                model_path=model_file,\n",
    "                owl_file=owl_file,\n",
    "                output_file=embedding_file\n",
    "            )\n",
    "        \n",
    "        # Load and process embeddings\n",
    "        df, metadata = load_embeddings_as_dataframe(embedding_file, return_metadata=True)\n",
    "        \n",
    "        results[role] = {\n",
    "            'name': name,\n",
    "            'file': owl_file,\n",
    "            'domain': ont_info['domain'],\n",
    "            'df': df,\n",
    "            'metadata': metadata,\n",
    "            'embeddings': np.stack(df['embedding'].to_numpy()),\n",
    "            'node_ids': df['node_id'].to_numpy(),\n",
    "            'embedding_file': embedding_file,\n",
    "            'model_file': model_file\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì {name}: {len(df)} concepts with {results[role]['embeddings'].shape[1]}D embeddings\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prepare embeddings for alignment\n",
    "print(\"üöÄ Preparing ontologies for alignment...\")\n",
    "alignment_data = prepare_alignment_embeddings(alignment_ontologies)\n",
    "\n",
    "if len(alignment_data) >= 2:\n",
    "    print(f\"\\n‚úÖ Ready for alignment with {len(alignment_data)} ontologies!\")\n",
    "    for role, data in alignment_data.items():\n",
    "        print(f\"  ‚Ä¢ {data['name']:8} ({role:6}): {len(data['node_ids']):,} concepts\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Need at least 2 ontologies for alignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Ontology Alignment System\n",
    "\n",
    "Create a comprehensive system for finding and scoring concept alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyAligner:\n",
    "    def __init__(self, source_data, target_data):\n",
    "        \"\"\"Initialize alignment system with source and target ontologies.\"\"\"\n",
    "        self.source = source_data\n",
    "        self.target = target_data\n",
    "        \n",
    "        print(f\"üîó Initializing alignment: {source_data['name']} ‚Üí {target_data['name']}\")\n",
    "        print(f\"  Source: {len(source_data['node_ids']):,} concepts\")\n",
    "        print(f\"  Target: {len(target_data['node_ids']):,} concepts\")\n",
    "        \n",
    "        # Pre-compute similarity matrices for efficiency\n",
    "        print(\"  Computing similarity matrix...\")\n",
    "        self.similarity_matrix = cosine_similarity(\n",
    "            source_data['embeddings'],\n",
    "            target_data['embeddings']\n",
    "        )\n",
    "        print(f\"  ‚úì Similarity matrix: {self.similarity_matrix.shape}\")\n",
    "        \n",
    "        # Create concept name indices for text-based matching\n",
    "        self.source_names = self._extract_concept_names(source_data['node_ids'])\n",
    "        self.target_names = self._extract_concept_names(target_data['node_ids'])\n",
    "    \n",
    "    def _extract_concept_names(self, node_ids):\n",
    "        \"\"\"Extract readable concept names from IRIs.\"\"\"\n",
    "        names = []\n",
    "        for node_id in node_ids:\n",
    "            # Extract name from IRI\n",
    "            if '#' in node_id:\n",
    "                name = node_id.split('#')[-1]\n",
    "            else:\n",
    "                name = node_id.split('/')[-1]\n",
    "            \n",
    "            # Clean up the name\n",
    "            name = name.replace('_', ' ').replace('-', ' ').lower().strip()\n",
    "            names.append(name)\n",
    "        return names\n",
    "    \n",
    "    def find_equivalence_mappings(self, similarity_threshold=0.8, max_mappings_per_concept=3):\n",
    "        \"\"\"Find equivalent concepts between ontologies.\"\"\"\n",
    "        mappings = []\n",
    "        \n",
    "        print(f\"üéØ Finding equivalence mappings (threshold: {similarity_threshold})...\")\n",
    "        \n",
    "        for i, source_id in enumerate(self.source['node_ids']):\n",
    "            # Find most similar target concepts\n",
    "            similarities = self.similarity_matrix[i]\n",
    "            top_indices = np.argsort(similarities)[::-1][:max_mappings_per_concept]\n",
    "            \n",
    "            for j in top_indices:\n",
    "                similarity = similarities[j]\n",
    "                if similarity >= similarity_threshold:\n",
    "                    target_id = self.target['node_ids'][j]\n",
    "                    \n",
    "                    # Add text similarity for validation\n",
    "                    text_similarity = self._text_similarity(\n",
    "                        self.source_names[i], \n",
    "                        self.target_names[j]\n",
    "                    )\n",
    "                    \n",
    "                    mappings.append({\n",
    "                        'source_id': source_id,\n",
    "                        'target_id': target_id,\n",
    "                        'source_name': self.source_names[i],\n",
    "                        'target_name': self.target_names[j],\n",
    "                        'embedding_similarity': float(similarity),\n",
    "                        'text_similarity': text_similarity,\n",
    "                        'combined_score': 0.8 * similarity + 0.2 * text_similarity,\n",
    "                        'mapping_type': 'equivalence',\n",
    "                        'confidence': self._calculate_confidence(similarity, text_similarity)\n",
    "                    })\n",
    "        \n",
    "        print(f\"  ‚úì Found {len(mappings)} equivalence mappings\")\n",
    "        return sorted(mappings, key=lambda x: x['combined_score'], reverse=True)\n",
    "    \n",
    "    def find_subsumption_mappings(self, similarity_threshold=0.6, subsumption_threshold=0.75):\n",
    "        \"\"\"Find subsumption relationships (broader/narrower concepts).\"\"\"\n",
    "        mappings = []\n",
    "        \n",
    "        print(f\"üîç Finding subsumption mappings...\")\n",
    "        \n",
    "        for i, source_id in enumerate(self.source['node_ids']):\n",
    "            source_embedding = self.source['embeddings'][i]\n",
    "            \n",
    "            for j, target_id in enumerate(self.target['node_ids']):\n",
    "                target_embedding = self.target['embeddings'][j]\n",
    "                \n",
    "                # Calculate directional similarities\n",
    "                similarity = self.similarity_matrix[i, j]\n",
    "                \n",
    "                if similarity >= similarity_threshold:\n",
    "                    # Check for subsumption patterns using embedding norms and clustering\n",
    "                    source_norm = np.linalg.norm(source_embedding)\n",
    "                    target_norm = np.linalg.norm(target_embedding)\n",
    "                    norm_ratio = source_norm / target_norm if target_norm > 0 else 1.0\n",
    "                    \n",
    "                    # Heuristic: concepts with higher norms tend to be more general\n",
    "                    if norm_ratio > 1.1 and similarity > subsumption_threshold:\n",
    "                        mapping_type = 'source_broader'  # source subsumes target\n",
    "                    elif norm_ratio < 0.9 and similarity > subsumption_threshold:\n",
    "                        mapping_type = 'target_broader'  # target subsumes source\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    text_similarity = self._text_similarity(\n",
    "                        self.source_names[i], \n",
    "                        self.target_names[j]\n",
    "                    )\n",
    "                    \n",
    "                    mappings.append({\n",
    "                        'source_id': source_id,\n",
    "                        'target_id': target_id,\n",
    "                        'source_name': self.source_names[i],\n",
    "                        'target_name': self.target_names[j],\n",
    "                        'embedding_similarity': float(similarity),\n",
    "                        'text_similarity': text_similarity,\n",
    "                        'norm_ratio': float(norm_ratio),\n",
    "                        'mapping_type': mapping_type,\n",
    "                        'combined_score': 0.7 * similarity + 0.3 * text_similarity,\n",
    "                        'confidence': self._calculate_confidence(similarity, text_similarity) * 0.8\n",
    "                    })\n",
    "        \n",
    "        print(f\"  ‚úì Found {len(mappings)} subsumption mappings\")\n",
    "        return sorted(mappings, key=lambda x: x['combined_score'], reverse=True)\n",
    "    \n",
    "    def _text_similarity(self, name1, name2):\n",
    "        \"\"\"Calculate text-based similarity between concept names.\"\"\"\n",
    "        # Simple text similarity based on common words\n",
    "        words1 = set(name1.split())\n",
    "        words2 = set(name2.split())\n",
    "        \n",
    "        if len(words1) == 0 or len(words2) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        jaccard = intersection / union if union > 0 else 0.0\n",
    "        \n",
    "        # Bonus for exact matches or substrings\n",
    "        if name1 == name2:\n",
    "            return 1.0\n",
    "        elif name1 in name2 or name2 in name1:\n",
    "            return max(0.8, jaccard)\n",
    "        \n",
    "        return jaccard\n",
    "    \n",
    "    def _calculate_confidence(self, embedding_sim, text_sim):\n",
    "        \"\"\"Calculate confidence score for a mapping.\"\"\"\n",
    "        # High confidence when both embedding and text similarities are high\n",
    "        if embedding_sim > 0.9 and text_sim > 0.5:\n",
    "            return 'high'\n",
    "        elif embedding_sim > 0.8 or text_sim > 0.7:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    \n",
    "    def create_alignment_clusters(self, all_mappings, cluster_threshold=0.7):\n",
    "        \"\"\"Group related mappings into alignment clusters.\"\"\"\n",
    "        print(f\"üé™ Creating alignment clusters...\")\n",
    "        \n",
    "        # Build similarity matrix for mappings\n",
    "        mapping_similarities = []\n",
    "        \n",
    "        for i, mapping1 in enumerate(all_mappings):\n",
    "            similarities_row = []\n",
    "            for j, mapping2 in enumerate(all_mappings):\n",
    "                if i == j:\n",
    "                    sim = 1.0\n",
    "                else:\n",
    "                    # Calculate similarity between mappings based on concept similarity\n",
    "                    source1_idx = list(self.source['node_ids']).index(mapping1['source_id'])\n",
    "                    source2_idx = list(self.source['node_ids']).index(mapping2['source_id'])\n",
    "                    \n",
    "                    target1_idx = list(self.target['node_ids']).index(mapping1['target_id'])\n",
    "                    target2_idx = list(self.target['node_ids']).index(mapping2['target_id'])\n",
    "                    \n",
    "                    # Similarity between source concepts\n",
    "                    source_sim = cosine_similarity(\n",
    "                        self.source['embeddings'][source1_idx].reshape(1, -1),\n",
    "                        self.source['embeddings'][source2_idx].reshape(1, -1)\n",
    "                    )[0, 0]\n",
    "                    \n",
    "                    # Similarity between target concepts\n",
    "                    target_sim = cosine_similarity(\n",
    "                        self.target['embeddings'][target1_idx].reshape(1, -1),\n",
    "                        self.target['embeddings'][target2_idx].reshape(1, -1)\n",
    "                    )[0, 0]\n",
    "                    \n",
    "                    sim = (source_sim + target_sim) / 2\n",
    "                \n",
    "                similarities_row.append(sim)\n",
    "            mapping_similarities.append(similarities_row)\n",
    "        \n",
    "        mapping_similarities = np.array(mapping_similarities)\n",
    "        \n",
    "        # Cluster mappings\n",
    "        distances = 1 - mapping_similarities\n",
    "        clustering = AgglomerativeClustering(\n",
    "            n_clusters=None,\n",
    "            distance_threshold=1-cluster_threshold,\n",
    "            linkage='average'\n",
    "        )\n",
    "        \n",
    "        cluster_labels = clustering.fit_predict(distances)\n",
    "        \n",
    "        # Group mappings by cluster\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(all_mappings[i])\n",
    "        \n",
    "        print(f\"  ‚úì Created {len(clusters)} alignment clusters\")\n",
    "        return clusters\n",
    "    \n",
    "    def validate_alignments(self, mappings, validation_method='reciprocal'):\n",
    "        \"\"\"Validate alignment mappings using different strategies.\"\"\"\n",
    "        print(f\"‚úÖ Validating alignments using {validation_method} method...\")\n",
    "        \n",
    "        validated_mappings = []\n",
    "        \n",
    "        if validation_method == 'reciprocal':\n",
    "            # Check if alignments are reciprocal (mutual best matches)\n",
    "            for mapping in mappings:\n",
    "                source_idx = list(self.source['node_ids']).index(mapping['source_id'])\n",
    "                target_idx = list(self.target['node_ids']).index(mapping['target_id'])\n",
    "                \n",
    "                # Check if target concept's best match is the source concept\n",
    "                target_to_source_similarities = self.similarity_matrix[:, target_idx]\n",
    "                best_source_idx = np.argmax(target_to_source_similarities)\n",
    "                \n",
    "                is_reciprocal = best_source_idx == source_idx\n",
    "                \n",
    "                mapping['is_reciprocal'] = is_reciprocal\n",
    "                mapping['validation_score'] = mapping['combined_score'] * (1.2 if is_reciprocal else 0.8)\n",
    "                \n",
    "                validated_mappings.append(mapping)\n",
    "        \n",
    "        elif validation_method == 'consistency':\n",
    "            # Check consistency with other mappings\n",
    "            for mapping in mappings:\n",
    "                consistency_score = self._calculate_consistency(mapping, mappings)\n",
    "                mapping['consistency_score'] = consistency_score\n",
    "                mapping['validation_score'] = mapping['combined_score'] * consistency_score\n",
    "                validated_mappings.append(mapping)\n",
    "        \n",
    "        validated_mappings.sort(key=lambda x: x['validation_score'], reverse=True)\n",
    "        return validated_mappings\n",
    "    \n",
    "    def _calculate_consistency(self, target_mapping, all_mappings):\n",
    "        \"\"\"Calculate consistency score with other mappings.\"\"\"\n",
    "        # Simple consistency: mappings of similar concepts should have similar targets\n",
    "        consistency_scores = []\n",
    "        \n",
    "        for other_mapping in all_mappings:\n",
    "            if other_mapping['source_id'] == target_mapping['source_id']:\n",
    "                continue\n",
    "            \n",
    "            # Calculate source concept similarity\n",
    "            source1_idx = list(self.source['node_ids']).index(target_mapping['source_id'])\n",
    "            source2_idx = list(self.source['node_ids']).index(other_mapping['source_id'])\n",
    "            \n",
    "            source_sim = cosine_similarity(\n",
    "                self.source['embeddings'][source1_idx].reshape(1, -1),\n",
    "                self.source['embeddings'][source2_idx].reshape(1, -1)\n",
    "            )[0, 0]\n",
    "            \n",
    "            if source_sim > 0.5:  # Only consider related source concepts\n",
    "                # Calculate target concept similarity\n",
    "                target1_idx = list(self.target['node_ids']).index(target_mapping['target_id'])\n",
    "                target2_idx = list(self.target['node_ids']).index(other_mapping['target_id'])\n",
    "                \n",
    "                target_sim = cosine_similarity(\n",
    "                    self.target['embeddings'][target1_idx].reshape(1, -1),\n",
    "                    self.target['embeddings'][target2_idx].reshape(1, -1)\n",
    "                )[0, 0]\n",
    "                \n",
    "                # Consistency: similar sources should map to similar targets\n",
    "                consistency = target_sim * source_sim\n",
    "                consistency_scores.append(consistency)\n",
    "        \n",
    "        return np.mean(consistency_scores) if consistency_scores else 0.5\n",
    "\n",
    "# Initialize alignment system\n",
    "if len(alignment_data) >= 2:\n",
    "    aligner = OntologyAligner(\n",
    "        alignment_data['source'],\n",
    "        alignment_data['target']\n",
    "    )\n",
    "    print(\"\\n‚úÖ Ontology alignment system ready!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Cannot initialize aligner - need both source and target ontologies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate and Analyze Alignments\n",
    "\n",
    "Let's find different types of alignments and analyze their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate alignments\n",
    "if 'aligner' in locals():\n",
    "    print(\"üîç Generating ontology alignments...\\n\")\n",
    "    \n",
    "    # Find equivalence mappings\n",
    "    equivalence_mappings = aligner.find_equivalence_mappings(\n",
    "        similarity_threshold=0.8,\n",
    "        max_mappings_per_concept=3\n",
    "    )\n",
    "    \n",
    "    # Find subsumption mappings\n",
    "    subsumption_mappings = aligner.find_subsumption_mappings(\n",
    "        similarity_threshold=0.6,\n",
    "        subsumption_threshold=0.75\n",
    "    )\n",
    "    \n",
    "    # Combine all mappings\n",
    "    all_mappings = equivalence_mappings + subsumption_mappings\n",
    "    \n",
    "    print(f\"\\nüìä Alignment Summary:\")\n",
    "    print(f\"  ‚Ä¢ Equivalence mappings: {len(equivalence_mappings)}\")\n",
    "    print(f\"  ‚Ä¢ Subsumption mappings: {len(subsumption_mappings)}\")\n",
    "    print(f\"  ‚Ä¢ Total mappings: {len(all_mappings)}\")\n",
    "    \n",
    "    if all_mappings:\n",
    "        # Validate alignments\n",
    "        validated_mappings = aligner.validate_alignments(\n",
    "            equivalence_mappings,  # Validate equivalence mappings\n",
    "            validation_method='reciprocal'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Validation Results:\")\n",
    "        reciprocal_count = sum(1 for m in validated_mappings if m.get('is_reciprocal', False))\n",
    "        print(f\"  ‚Ä¢ Reciprocal mappings: {reciprocal_count}/{len(validated_mappings)} ({reciprocal_count/len(validated_mappings)*100:.1f}%)\")\n",
    "        \n",
    "        # Show top alignments\n",
    "        print(f\"\\nüèÜ Top Equivalence Alignments:\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for i, mapping in enumerate(validated_mappings[:10]):\n",
    "            reciprocal_mark = \"‚ÜîÔ∏è\" if mapping.get('is_reciprocal', False) else \"‚Üí\"\n",
    "            confidence_mark = {\"high\": \"üü¢\", \"medium\": \"üü°\", \"low\": \"üî¥\"}.get(mapping['confidence'], \"‚ö™\")\n",
    "            \n",
    "            print(f\"{confidence_mark} {i+1:2d}. {mapping['source_name']:30} {reciprocal_mark} {mapping['target_name']:30}\")\n",
    "            print(f\"     Embedding: {mapping['embedding_similarity']:.3f} | Text: {mapping['text_similarity']:.3f} | Combined: {mapping['combined_score']:.3f}\")\n",
    "            print()\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No alignments found with current thresholds\")\n",
    "\n",
    "else:\n",
    "    print(\"Aligner not available - please run previous cells first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Alignment Network Visualization\n",
    "\n",
    "Visualize the alignment network to understand the relationships between ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignment_network_viz(aligner, mappings, top_k=50):\n",
    "    \"\"\"Create interactive network visualization of alignments.\"\"\"\n",
    "    if not mappings:\n",
    "        print(\"No mappings to visualize\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üï∏Ô∏è Creating alignment network visualization...\")\n",
    "    \n",
    "    # Select top mappings for visualization\n",
    "    viz_mappings = mappings[:top_k]\n",
    "    \n",
    "    # Build network graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    source_nodes = set()\n",
    "    target_nodes = set()\n",
    "    \n",
    "    for mapping in viz_mappings:\n",
    "        source_id = f\"S_{mapping['source_name']}\"\n",
    "        target_id = f\"T_{mapping['target_name']}\"\n",
    "        \n",
    "        source_nodes.add(source_id)\n",
    "        target_nodes.add(target_id)\n",
    "        \n",
    "        # Add nodes with attributes\n",
    "        G.add_node(source_id, \n",
    "                  ontology='source', \n",
    "                  concept_name=mapping['source_name'],\n",
    "                  full_id=mapping['source_id'])\n",
    "        \n",
    "        G.add_node(target_id, \n",
    "                  ontology='target', \n",
    "                  concept_name=mapping['target_name'],\n",
    "                  full_id=mapping['target_id'])\n",
    "        \n",
    "        # Add edge with alignment attributes\n",
    "        G.add_edge(source_id, target_id,\n",
    "                  weight=mapping['combined_score'],\n",
    "                  embedding_similarity=mapping['embedding_similarity'],\n",
    "                  text_similarity=mapping['text_similarity'],\n",
    "                  mapping_type=mapping.get('mapping_type', 'equivalence'),\n",
    "                  confidence=mapping.get('confidence', 'medium'),\n",
    "                  is_reciprocal=mapping.get('is_reciprocal', False))\n",
    "    \n",
    "    # Layout the network\n",
    "    print(f\"  Laying out network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges...\")\n",
    "    \n",
    "    # Use spring layout with separation between ontologies\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    # Separate source and target nodes visually\n",
    "    for node in source_nodes:\n",
    "        if node in pos:\n",
    "            pos[node] = (pos[node][0] - 1, pos[node][1])\n",
    "    \n",
    "    for node in target_nodes:\n",
    "        if node in pos:\n",
    "            pos[node] = (pos[node][0] + 1, pos[node][1])\n",
    "    \n",
    "    # Create plotly visualization\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_info = []\n",
    "    \n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x.extend([x0, x1, None])\n",
    "        edge_y.extend([y0, y1, None])\n",
    "        \n",
    "        edge_data = G.edges[edge]\n",
    "        edge_info.append({\n",
    "            'source': edge[0].replace('S_', ''),\n",
    "            'target': edge[1].replace('T_', ''),\n",
    "            'weight': edge_data['weight'],\n",
    "            'confidence': edge_data['confidence']\n",
    "        })\n",
    "    \n",
    "    # Create edge trace\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=1, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "    \n",
    "    # Create node traces (separate for each ontology)\n",
    "    source_node_x = [pos[node][0] for node in source_nodes if node in pos]\n",
    "    source_node_y = [pos[node][1] for node in source_nodes if node in pos]\n",
    "    source_text = [node.replace('S_', '') for node in source_nodes if node in pos]\n",
    "    \n",
    "    target_node_x = [pos[node][0] for node in target_nodes if node in pos]\n",
    "    target_node_y = [pos[node][1] for node in target_nodes if node in pos]\n",
    "    target_text = [node.replace('T_', '') for node in target_nodes if node in pos]\n",
    "    \n",
    "    source_trace = go.Scatter(\n",
    "        x=source_node_x, y=source_node_y,\n",
    "        mode='markers+text',\n",
    "        text=source_text,\n",
    "        textposition=\"middle center\",\n",
    "        name=f\"Source ({aligner.source['name']})\",\n",
    "        marker=dict(\n",
    "            size=20,\n",
    "            color='lightblue',\n",
    "            line=dict(width=2, color='blue')\n",
    "        ),\n",
    "        hovertemplate='<b>%{text}</b><br>Ontology: ' + aligner.source['name'] + '<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    target_trace = go.Scatter(\n",
    "        x=target_node_x, y=target_node_y,\n",
    "        mode='markers+text',\n",
    "        text=target_text,\n",
    "        textposition=\"middle center\",\n",
    "        name=f\"Target ({aligner.target['name']})\",\n",
    "        marker=dict(\n",
    "            size=20,\n",
    "            color='lightcoral',\n",
    "            line=dict(width=2, color='red')\n",
    "        ),\n",
    "        hovertemplate='<b>%{text}</b><br>Ontology: ' + aligner.target['name'] + '<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=[edge_trace, source_trace, target_trace],\n",
    "                   layout=go.Layout(\n",
    "                        title=f'Ontology Alignment Network<br><sub>{aligner.source[\"name\"]} ‚Üî {aligner.target[\"name\"]} (Top {len(viz_mappings)} alignments)</sub>',\n",
    "                        titlefont_size=16,\n",
    "                        showlegend=True,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20,l=5,r=5,t=40),\n",
    "                        annotations=[ dict(\n",
    "                            text=\"Blue: Source ontology, Red: Target ontology, Lines: Alignments\",\n",
    "                            showarrow=False,\n",
    "                            xref=\"paper\", yref=\"paper\",\n",
    "                            x=0.005, y=-0.002 ) ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        width=1200,\n",
    "                        height=800\n",
    "                   ))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create network visualization\n",
    "if 'aligner' in locals() and 'validated_mappings' in locals() and validated_mappings:\n",
    "    network_fig = create_alignment_network_viz(aligner, validated_mappings, top_k=30)\n",
    "    if network_fig:\n",
    "        network_fig.show()\n",
    "else:\n",
    "    print(\"Network visualization not available - need validated mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Alignment Quality Analysis\n",
    "\n",
    "Analyze the quality and characteristics of the generated alignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_alignment_quality(mappings):\n",
    "    \"\"\"Comprehensive analysis of alignment quality.\"\"\"\n",
    "    if not mappings:\n",
    "        print(\"No mappings to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìà ALIGNMENT QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    df = pd.DataFrame(mappings)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä Basic Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total alignments: {len(mappings)}\")\n",
    "    print(f\"  ‚Ä¢ Unique source concepts: {df['source_id'].nunique()}\")\n",
    "    print(f\"  ‚Ä¢ Unique target concepts: {df['target_id'].nunique()}\")\n",
    "    \n",
    "    # Similarity distributions\n",
    "    print(f\"\\nüéØ Similarity Distributions:\")\n",
    "    print(f\"  ‚Ä¢ Embedding similarity: {df['embedding_similarity'].mean():.3f} ¬± {df['embedding_similarity'].std():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Text similarity: {df['text_similarity'].mean():.3f} ¬± {df['text_similarity'].std():.3f}\")\n",
    "    print(f\"  ‚Ä¢ Combined score: {df['combined_score'].mean():.3f} ¬± {df['combined_score'].std():.3f}\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    if 'confidence' in df.columns:\n",
    "        confidence_counts = df['confidence'].value_counts()\n",
    "        print(f\"\\nüéöÔ∏è Confidence Distribution:\")\n",
    "        for conf, count in confidence_counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"  ‚Ä¢ {conf:6}: {count:3d} ({pct:4.1f}%)\")\n",
    "    \n",
    "    # Mapping type distribution\n",
    "    if 'mapping_type' in df.columns:\n",
    "        type_counts = df['mapping_type'].value_counts()\n",
    "        print(f\"\\nüîó Mapping Type Distribution:\")\n",
    "        for mtype, count in type_counts.items():\n",
    "            pct = count / len(df) * 100\n",
    "            print(f\"  ‚Ä¢ {mtype:15}: {count:3d} ({pct:4.1f}%)\")\n",
    "    \n",
    "    # Reciprocal analysis\n",
    "    if 'is_reciprocal' in df.columns:\n",
    "        reciprocal_count = df['is_reciprocal'].sum()\n",
    "        print(f\"\\n‚ÜîÔ∏è Reciprocal Mappings:\")\n",
    "        print(f\"  ‚Ä¢ Reciprocal: {reciprocal_count} ({reciprocal_count/len(df)*100:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Non-reciprocal: {len(df)-reciprocal_count} ({(len(df)-reciprocal_count)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Similarity distributions\n",
    "    ax1.hist(df['embedding_similarity'], bins=20, alpha=0.7, label='Embedding', color='skyblue')\n",
    "    ax1.hist(df['text_similarity'], bins=20, alpha=0.7, label='Text', color='lightcoral')\n",
    "    ax1.hist(df['combined_score'], bins=20, alpha=0.7, label='Combined', color='lightgreen')\n",
    "    ax1.set_title('Similarity Score Distributions')\n",
    "    ax1.set_xlabel('Similarity Score')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: Embedding vs Text similarity\n",
    "    scatter = ax2.scatter(df['embedding_similarity'], df['text_similarity'], \n",
    "                         c=df['combined_score'], cmap='viridis', alpha=0.6)\n",
    "    ax2.set_title('Embedding vs Text Similarity')\n",
    "    ax2.set_xlabel('Embedding Similarity')\n",
    "    ax2.set_ylabel('Text Similarity')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Combined Score')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    if 'confidence' in df.columns:\n",
    "        confidence_counts.plot(kind='bar', ax=ax3, color=['red', 'orange', 'green'])\n",
    "        ax3.set_title('Confidence Distribution')\n",
    "        ax3.set_xlabel('Confidence Level')\n",
    "        ax3.set_ylabel('Count')\n",
    "        ax3.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Top scoring alignments\n",
    "    top_scores = df.nlargest(15, 'combined_score')[['source_name', 'target_name', 'combined_score']]\n",
    "    y_pos = np.arange(len(top_scores))\n",
    "    \n",
    "    ax4.barh(y_pos, top_scores['combined_score'], color='gold')\n",
    "    ax4.set_yticks(y_pos)\n",
    "    ax4.set_yticklabels([f\"{row['source_name']} ‚Üí {row['target_name']}\" for _, row in top_scores.iterrows()], \n",
    "                       fontsize=8)\n",
    "    ax4.set_title('Top 15 Alignment Scores')\n",
    "    ax4.set_xlabel('Combined Score')\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return analysis summary\n",
    "    return {\n",
    "        'total_alignments': len(mappings),\n",
    "        'unique_source_concepts': df['source_id'].nunique(),\n",
    "        'unique_target_concepts': df['target_id'].nunique(),\n",
    "        'mean_embedding_similarity': df['embedding_similarity'].mean(),\n",
    "        'mean_text_similarity': df['text_similarity'].mean(),\n",
    "        'mean_combined_score': df['combined_score'].mean(),\n",
    "        'high_confidence_count': df[df['confidence'] == 'high'].shape[0] if 'confidence' in df.columns else 0,\n",
    "        'reciprocal_count': df['is_reciprocal'].sum() if 'is_reciprocal' in df.columns else 0\n",
    "    }\n",
    "\n",
    "# Analyze alignment quality\n",
    "if 'validated_mappings' in locals() and validated_mappings:\n",
    "    analysis_results = analyze_alignment_quality(validated_mappings)\n",
    "else:\n",
    "    print(\"No validated mappings available for quality analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export Alignment Results\n",
    "\n",
    "Export alignments in standard formats for use in other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_alignments(mappings, aligner, output_format='all', output_dir='alignment_results'):\n",
    "    \"\"\"Export alignments in various standard formats.\"\"\"\n",
    "    if not mappings:\n",
    "        print(\"No mappings to export\")\n",
    "        return\n",
    "    \n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    source_name = aligner.source['name']\n",
    "    target_name = aligner.target['name']\n",
    "    base_filename = f\"{source_name}_to_{target_name}_alignment\"\n",
    "    \n",
    "    exported_files = []\n",
    "    \n",
    "    print(f\"üìÅ Exporting alignments to {output_dir}/...\")\n",
    "    \n",
    "    # 1. CSV format (human readable)\n",
    "    if output_format in ['csv', 'all']:\n",
    "        csv_file = Path(output_dir) / f\"{base_filename}.csv\"\n",
    "        df = pd.DataFrame(mappings)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        exported_files.append(str(csv_file))\n",
    "        print(f\"  ‚úì CSV: {csv_file}\")\n",
    "    \n",
    "    # 2. RDF alignment format (standard)\n",
    "    if output_format in ['rdf', 'all']:\n",
    "        rdf_file = Path(output_dir) / f\"{base_filename}.rdf\"\n",
    "        with open(rdf_file, 'w') as f:\n",
    "            f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "            f.write('<rdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\\n')\n",
    "            f.write('         xmlns:align=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment#\">\\n\\n')\n",
    "            f.write('<align:Alignment>\\n')\n",
    "            f.write(f'  <align:onto1>{aligner.source[\"name\"]}</align:onto1>\\n')\n",
    "            f.write(f'  <align:onto2>{aligner.target[\"name\"]}</align:onto2>\\n')\n",
    "            f.write('  <align:map>\\n')\n",
    "            \n",
    "            for i, mapping in enumerate(mappings):\n",
    "                relation = '=' if mapping.get('mapping_type', 'equivalence') == 'equivalence' else '&lt;'\n",
    "                confidence = mapping.get('combined_score', 0.5)\n",
    "                \n",
    "                f.write(f'    <align:Cell rdf:about=\"#{i}\">\\n')\n",
    "                f.write(f'      <align:entity1 rdf:resource=\"{mapping[\"source_id\"]}\"/>\\n')\n",
    "                f.write(f'      <align:entity2 rdf:resource=\"{mapping[\"target_id\"]}\"/>\\n')\n",
    "                f.write(f'      <align:relation>{relation}</align:relation>\\n')\n",
    "                f.write(f'      <align:measure rdf:datatype=\"xsd:float\">{confidence:.3f}</align:measure>\\n')\n",
    "                f.write(f'    </align:Cell>\\n')\n",
    "            \n",
    "            f.write('  </align:map>\\n')\n",
    "            f.write('</align:Alignment>\\n')\n",
    "            f.write('</rdf:RDF>\\n')\n",
    "        \n",
    "        exported_files.append(str(rdf_file))\n",
    "        print(f\"  ‚úì RDF: {rdf_file}\")\n",
    "    \n",
    "    # 3. EDOAL format (expressive alignment format)\n",
    "    if output_format in ['edoal', 'all']:\n",
    "        edoal_file = Path(output_dir) / f\"{base_filename}.edoal\"\n",
    "        with open(edoal_file, 'w') as f:\n",
    "            f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "            f.write('<edoal:Alignment xmlns:edoal=\"http://ns.inria.org/edoal/1.0#\"\\n')\n",
    "            f.write('                xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\\n')\n",
    "            f.write(f'  <edoal:onto1>{aligner.source[\"name\"]}</edoal:onto1>\\n')\n",
    "            f.write(f'  <edoal:onto2>{aligner.target[\"name\"]}</edoal:onto2>\\n')\n",
    "            \n",
    "            for i, mapping in enumerate(mappings):\n",
    "                f.write(f'  <edoal:map>\\n')\n",
    "                f.write(f'    <edoal:Cell rdf:about=\"#{i}\">\\n')\n",
    "                f.write(f'      <edoal:entity1>\\n')\n",
    "                f.write(f'        <edoal:Class rdf:about=\"{mapping[\"source_id\"]}\"/>\\n')\n",
    "                f.write(f'      </edoal:entity1>\\n')\n",
    "                f.write(f'      <edoal:entity2>\\n')\n",
    "                f.write(f'        <edoal:Class rdf:about=\"{mapping[\"target_id\"]}\"/>\\n')\n",
    "                f.write(f'      </edoal:entity2>\\n')\n",
    "                f.write(f'      <edoal:relation>=</edoal:relation>\\n')\n",
    "                f.write(f'      <edoal:measure>{mapping.get(\"combined_score\", 0.5):.3f}</edoal:measure>\\n')\n",
    "                f.write(f'    </edoal:Cell>\\n')\n",
    "                f.write(f'  </edoal:map>\\n')\n",
    "            \n",
    "            f.write('</edoal:Alignment>\\n')\n",
    "        \n",
    "        exported_files.append(str(edoal_file))\n",
    "        print(f\"  ‚úì EDOAL: {edoal_file}\")\n",
    "    \n",
    "    # 4. JSON format (for programmatic use)\n",
    "    if output_format in ['json', 'all']:\n",
    "        json_file = Path(output_dir) / f\"{base_filename}.json\"\n",
    "        \n",
    "        alignment_data = {\n",
    "            'metadata': {\n",
    "                'source_ontology': aligner.source['name'],\n",
    "                'target_ontology': aligner.target['name'],\n",
    "                'source_file': aligner.source['file'],\n",
    "                'target_file': aligner.target['file'],\n",
    "                'total_mappings': len(mappings),\n",
    "                'generation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "                'tool': 'on2vec'\n",
    "            },\n",
    "            'mappings': mappings\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(alignment_data, f, indent=2, default=str)\n",
    "        \n",
    "        exported_files.append(str(json_file))\n",
    "        print(f\"  ‚úì JSON: {json_file}\")\n",
    "    \n",
    "    # 5. Summary report\n",
    "    if output_format in ['report', 'all']:\n",
    "        report_file = Path(output_dir) / f\"{base_filename}_report.txt\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(f\"ONTOLOGY ALIGNMENT REPORT\\n\")\n",
    "            f.write(f\"{'='*50}\\n\\n\")\n",
    "            f.write(f\"Source Ontology: {aligner.source['name']} ({aligner.source['file']})\\n\")\n",
    "            f.write(f\"Target Ontology: {aligner.target['name']} ({aligner.target['file']})\\n\")\n",
    "            f.write(f\"Generated: {pd.Timestamp.now()}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"ALIGNMENT STATISTICS\\n\")\n",
    "            f.write(f\"{'-'*30}\\n\")\n",
    "            f.write(f\"Total mappings: {len(mappings)}\\n\")\n",
    "            \n",
    "            df = pd.DataFrame(mappings)\n",
    "            f.write(f\"Mean embedding similarity: {df['embedding_similarity'].mean():.3f}\\n\")\n",
    "            f.write(f\"Mean text similarity: {df['text_similarity'].mean():.3f}\\n\")\n",
    "            f.write(f\"Mean combined score: {df['combined_score'].mean():.3f}\\n\\n\")\n",
    "            \n",
    "            # Top 20 mappings\n",
    "            f.write(f\"TOP 20 ALIGNMENTS\\n\")\n",
    "            f.write(f\"{'-'*30}\\n\")\n",
    "            for i, mapping in enumerate(sorted(mappings, key=lambda x: x['combined_score'], reverse=True)[:20]):\n",
    "                f.write(f\"{i+1:2d}. {mapping['source_name']:30} ‚Üí {mapping['target_name']:30} ({mapping['combined_score']:.3f})\\n\")\n",
    "        \n",
    "        exported_files.append(str(report_file))\n",
    "        print(f\"  ‚úì Report: {report_file}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Exported {len(exported_files)} files to {output_dir}/\")\n",
    "    return exported_files\n",
    "\n",
    "# Export alignments\n",
    "if 'validated_mappings' in locals() and validated_mappings and 'aligner' in locals():\n",
    "    exported_files = export_alignments(\n",
    "        validated_mappings,\n",
    "        aligner,\n",
    "        output_format='all'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìã Export Summary:\")\n",
    "    for file_path in exported_files:\n",
    "        file_size = Path(file_path).stat().st_size\n",
    "        print(f\"  ‚Ä¢ {Path(file_path).name}: {file_size:,} bytes\")\n",
    "        \n",
    "else:\n",
    "    print(\"No alignments available for export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cross-Ontology Query Example\n",
    "\n",
    "Demonstrate how alignments enable cross-ontology queries and knowledge integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossOntologyQuerier:\n",
    "    def __init__(self, aligner, alignments):\n",
    "        \"\"\"Initialize cross-ontology query system.\"\"\"\n",
    "        self.aligner = aligner\n",
    "        self.alignments = alignments\n",
    "        \n",
    "        # Build bidirectional alignment index\n",
    "        self.source_to_target = {}\n",
    "        self.target_to_source = {}\n",
    "        \n",
    "        for alignment in alignments:\n",
    "            source_id = alignment['source_id']\n",
    "            target_id = alignment['target_id']\n",
    "            score = alignment['combined_score']\n",
    "            \n",
    "            if source_id not in self.source_to_target:\n",
    "                self.source_to_target[source_id] = []\n",
    "            self.source_to_target[source_id].append((target_id, score, alignment))\n",
    "            \n",
    "            if target_id not in self.target_to_source:\n",
    "                self.target_to_source[target_id] = []\n",
    "            self.target_to_source[target_id].append((source_id, score, alignment))\n",
    "        \n",
    "        print(f\"üîç Cross-ontology querier ready with {len(alignments)} alignments\")\n",
    "    \n",
    "    def translate_concept(self, concept_id, from_ontology='auto'):\n",
    "        \"\"\"Translate a concept to the other ontology.\"\"\"\n",
    "        translations = []\n",
    "        \n",
    "        if from_ontology == 'auto':\n",
    "            # Try both directions\n",
    "            if concept_id in self.source_to_target:\n",
    "                from_ontology = 'source'\n",
    "            elif concept_id in self.target_to_source:\n",
    "                from_ontology = 'target'\n",
    "            else:\n",
    "                return translations\n",
    "        \n",
    "        if from_ontology == 'source' and concept_id in self.source_to_target:\n",
    "            for target_id, score, alignment in self.source_to_target[concept_id]:\n",
    "                translations.append({\n",
    "                    'original_id': concept_id,\n",
    "                    'original_name': alignment['source_name'],\n",
    "                    'original_ontology': self.aligner.source['name'],\n",
    "                    'translated_id': target_id,\n",
    "                    'translated_name': alignment['target_name'],\n",
    "                    'translated_ontology': self.aligner.target['name'],\n",
    "                    'confidence': score,\n",
    "                    'alignment_type': alignment.get('mapping_type', 'equivalence')\n",
    "                })\n",
    "        \n",
    "        elif from_ontology == 'target' and concept_id in self.target_to_source:\n",
    "            for source_id, score, alignment in self.target_to_source[concept_id]:\n",
    "                translations.append({\n",
    "                    'original_id': concept_id,\n",
    "                    'original_name': alignment['target_name'],\n",
    "                    'original_ontology': self.aligner.target['name'],\n",
    "                    'translated_id': source_id,\n",
    "                    'translated_name': alignment['source_name'],\n",
    "                    'translated_ontology': self.aligner.source['name'],\n",
    "                    'confidence': score,\n",
    "                    'alignment_type': alignment.get('mapping_type', 'equivalence')\n",
    "                })\n",
    "        \n",
    "        return sorted(translations, key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    def cross_ontology_search(self, query_terms, min_confidence=0.6):\n",
    "        \"\"\"Search for concepts across both ontologies using alignments.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Search in source ontology\n",
    "        source_matches = []\n",
    "        for i, name in enumerate(self.aligner.source_names):\n",
    "            if any(term.lower() in name.lower() for term in query_terms):\n",
    "                source_id = self.aligner.source['node_ids'][i]\n",
    "                source_matches.append({\n",
    "                    'id': source_id,\n",
    "                    'name': name,\n",
    "                    'ontology': self.aligner.source['name'],\n",
    "                    'type': 'direct_match'\n",
    "                })\n",
    "        \n",
    "        # Search in target ontology\n",
    "        target_matches = []\n",
    "        for i, name in enumerate(self.aligner.target_names):\n",
    "            if any(term.lower() in name.lower() for term in query_terms):\n",
    "                target_id = self.aligner.target['node_ids'][i]\n",
    "                target_matches.append({\n",
    "                    'id': target_id,\n",
    "                    'name': name,\n",
    "                    'ontology': self.aligner.target['name'],\n",
    "                    'type': 'direct_match'\n",
    "                })\n",
    "        \n",
    "        # Add translations for source matches\n",
    "        for match in source_matches:\n",
    "            results.append(match)\n",
    "            translations = self.translate_concept(match['id'], 'source')\n",
    "            for trans in translations:\n",
    "                if trans['confidence'] >= min_confidence:\n",
    "                    results.append({\n",
    "                        'id': trans['translated_id'],\n",
    "                        'name': trans['translated_name'],\n",
    "                        'ontology': trans['translated_ontology'],\n",
    "                        'type': 'aligned_match',\n",
    "                        'source_match': match['name'],\n",
    "                        'confidence': trans['confidence']\n",
    "                    })\n",
    "        \n",
    "        # Add translations for target matches\n",
    "        for match in target_matches:\n",
    "            results.append(match)\n",
    "            translations = self.translate_concept(match['id'], 'target')\n",
    "            for trans in translations:\n",
    "                if trans['confidence'] >= min_confidence:\n",
    "                    results.append({\n",
    "                        'id': trans['translated_id'],\n",
    "                        'name': trans['translated_name'],\n",
    "                        'ontology': trans['translated_ontology'],\n",
    "                        'type': 'aligned_match',\n",
    "                        'source_match': match['name'],\n",
    "                        'confidence': trans['confidence']\n",
    "                    })\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        seen_ids = set()\n",
    "        unique_results = []\n",
    "        for result in results:\n",
    "            if result['id'] not in seen_ids:\n",
    "                unique_results.append(result)\n",
    "                seen_ids.add(result['id'])\n",
    "        \n",
    "        return unique_results\n",
    "\n",
    "# Initialize cross-ontology querier\n",
    "if 'validated_mappings' in locals() and validated_mappings and 'aligner' in locals():\n",
    "    querier = CrossOntologyQuerier(aligner, validated_mappings)\n",
    "    \n",
    "    # Example queries\n",
    "    print(\"\\nüîç CROSS-ONTOLOGY QUERY EXAMPLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    example_queries = [\"protein\", \"data\", \"structure\", \"analysis\"]\n",
    "    \n",
    "    for query in example_queries:\n",
    "        print(f\"\\nüìù Query: '{query}'\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        results = querier.cross_ontology_search([query], min_confidence=0.6)\n",
    "        \n",
    "        if results:\n",
    "            for i, result in enumerate(results[:8]):\n",
    "                type_mark = \"üéØ\" if result['type'] == 'direct_match' else \"üîó\"\n",
    "                ont_mark = \"üîµ\" if result['ontology'] == aligner.source['name'] else \"üî¥\"\n",
    "                \n",
    "                print(f\"{type_mark} {ont_mark} {result['name']:35} ({result['ontology']})\")\n",
    "                if result['type'] == 'aligned_match':\n",
    "                    print(f\"     ‚Ü≥ via alignment from '{result['source_match']}' (conf: {result['confidence']:.3f})\")\n",
    "        else:\n",
    "            print(\"  No results found\")\n",
    "    \n",
    "    # Example concept translation\n",
    "    print(f\"\\nüåâ CONCEPT TRANSLATION EXAMPLES\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Pick some concepts to translate\n",
    "    sample_source_ids = list(aligner.source['node_ids'])[:5]\n",
    "    \n",
    "    for concept_id in sample_source_ids:\n",
    "        translations = querier.translate_concept(concept_id, 'source')\n",
    "        if translations:\n",
    "            trans = translations[0]  # Best translation\n",
    "            print(f\"üîµ {trans['original_name']:30} ‚Üí\")\n",
    "            print(f\"üî¥ {trans['translated_name']:30} (conf: {trans['confidence']:.3f})\")\n",
    "            print()\n",
    "        \n",
    "else:\n",
    "    print(\"Cross-ontology querier not available - need validated mappings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated comprehensive ontology alignment capabilities using on2vec embeddings:\n",
    "\n",
    "### ‚úÖ Key Achievements:\n",
    "\n",
    "1. **Automated Alignment Discovery**: Found equivalent and subsumption relationships between ontologies\n",
    "2. **Multi-modal Similarity**: Combined embedding-based and text-based similarity for robust matching\n",
    "3. **Alignment Validation**: Used reciprocal matching and consistency checking for quality assurance\n",
    "4. **Interactive Visualization**: Created network graphs showing alignment relationships\n",
    "5. **Standard Format Export**: Generated alignments in RDF, EDOAL, CSV, and JSON formats\n",
    "6. **Cross-Ontology Queries**: Enabled seamless querying across different ontological vocabularies\n",
    "\n",
    "### üéØ Practical Applications:\n",
    "\n",
    "- **Data Integration**: Merge datasets using different ontological schemas\n",
    "- **Federated Search**: Query multiple knowledge bases with unified vocabulary\n",
    "- **Ontology Evolution**: Track concept changes across ontology versions\n",
    "- **Knowledge Graph Fusion**: Combine multiple domain-specific knowledge graphs\n",
    "- **Semantic Interoperability**: Enable communication between different systems\n",
    "\n",
    "### üìä Quality Metrics:\n",
    "\n",
    "- **Precision**: High-confidence alignments based on multi-modal similarity\n",
    "- **Recall**: Comprehensive coverage using embedding-based semantic matching\n",
    "- **Validation**: Reciprocal checking and consistency analysis\n",
    "- **Interpretability**: Text similarity provides explainable alignment rationale\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **Large-Scale Evaluation**: Test on standard alignment benchmarks (OAEI)\n",
    "2. **Multi-Way Alignment**: Extend to align multiple ontologies simultaneously\n",
    "3. **Interactive Refinement**: Build UI for manual alignment validation and correction\n",
    "4. **Domain-Specific Tuning**: Optimize for specific domains (biomedical, engineering, etc.)\n",
    "5. **Temporal Alignment**: Track concept alignments over time as ontologies evolve\n",
    "\n",
    "The automated alignment capabilities demonstrated here show how on2vec embeddings can solve real-world ontology integration challenges, enabling seamless knowledge sharing across different vocabularies and domains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}