#!/usr/bin/env python3
"""
HuggingFace integration workflows for on2vec.

This module contains all the HuggingFace Sentence Transformers integration
functionality that was previously in create_hf_model.py and batch_hf_models.py.
"""

import logging
from pathlib import Path
from typing import Optional, List, Dict, Any

from .workflows import train_and_embed_workflow
from .sentence_transformer_hub import create_and_save_hf_model
from .io import inspect_parquet_metadata
from .metadata_utils import get_base_model_from_embeddings, get_embedding_info, validate_text_embeddings_compatibility
from .model_card_generator import create_model_card, create_upload_instructions

logger = logging.getLogger(__name__)


def train_ontology_with_text(
    owl_file: str,
    output_file: str,
    text_model: str = "all-MiniLM-L6-v2",
    epochs: int = 100,
    model_type: str = "gcn",
    hidden_dim: int = 128,
    out_dim: int = 64,
    loss_fn: str = "triplet"
) -> bool:
    """Train an ontology model with text features enabled."""
    print(f"ğŸ§¬ Training ontology model: {owl_file}")
    print(f"ğŸ“Š Output: {output_file}")
    print(f"ğŸ¤– Text model: {text_model}")
    print(f"âš™ï¸ Config: {model_type}, hidden={hidden_dim}, out={out_dim}, loss={loss_fn}, epochs={epochs}")

    try:
        # Use the workflow function instead of subprocess
        result = train_and_embed_workflow(
            owl_file=owl_file,
            model_type=model_type,
            hidden_dim=hidden_dim,
            out_dim=out_dim,
            epochs=epochs,
            output=output_file,
            model_output=f"{Path(output_file).stem}_model.pt",
            loss_fn=loss_fn,
            use_text_features=True,
            text_model_name=text_model,
            fusion_method="concat"
        )

        print("âœ… Training completed successfully!")
        return True

    except Exception as e:
        print(f"âŒ Training failed: {e}")
        logger.error(f"Training failed: {e}")
        return False


def validate_embeddings(embeddings_file: str) -> Dict[str, Any]:
    """Validate embeddings file and return metadata."""
    print(f"ğŸ” Validating embeddings: {embeddings_file}")

    try:
        # Use the existing inspection function
        inspect_parquet_metadata(embeddings_file)

        # Get detailed metadata
        embedding_info = get_embedding_info(embeddings_file)

        print("âœ… Embeddings validation passed!")
        print(f"   Concepts: {embedding_info.get('num_embeddings', 'Unknown'):,}")
        print(f"   Text dim: {embedding_info.get('text_dim', 'Unknown')}")
        print(f"   Structural dim: {embedding_info.get('structural_dim', 'Unknown')}")

        return embedding_info

    except Exception as e:
        print(f"âŒ Validation failed: {e}")
        logger.error(f"Validation failed: {e}")
        raise


def create_hf_model(
    embeddings_file: str,
    model_name: str,
    output_dir: str = "./hf_models",
    base_model: Optional[str] = None,
    fusion_method: str = "concat",
    validate_first: bool = True,
    ontology_file: Optional[str] = None,
    training_config: Optional[Dict[str, Any]] = None
) -> str:
    """Create HuggingFace compatible model."""
    print(f"ğŸ—ï¸ Creating HuggingFace model: {model_name}")
    print(f"ğŸ“ Output directory: {output_dir}")
    print(f"ğŸ”— Fusion method: {fusion_method}")

    if validate_first:
        validate_embeddings(embeddings_file)

    # Auto-infer base model from embeddings if not provided
    if base_model is None:
        print("ğŸ” Auto-detecting base model from embeddings metadata...")
        inferred_model = get_base_model_from_embeddings(embeddings_file)
        if inferred_model:
            base_model = inferred_model
            print(f"âœ… Detected base model: {base_model}")
        else:
            base_model = "all-MiniLM-L6-v2"  # Default fallback
            print(f"âš ï¸  Could not detect base model, using default: {base_model}")
    else:
        # Validate compatibility if user specified a base model
        if not validate_text_embeddings_compatibility(embeddings_file, base_model):
            inferred_model = get_base_model_from_embeddings(embeddings_file)
            if inferred_model:
                print(f"âš ï¸  WARNING: Base model mismatch!")
                print(f"    Embeddings were created with: {inferred_model}")
                print(f"    You specified: {base_model}")
                print(f"    Using detected model: {inferred_model}")
                base_model = inferred_model
            else:
                print(f"âš ï¸  Could not verify base model compatibility, proceeding with: {base_model}")

    print(f"ğŸ¤– Using base model: {base_model}")

    try:
        model_path = create_and_save_hf_model(
            ontology_embeddings_file=embeddings_file,
            model_name=model_name,
            output_dir=output_dir,
            base_model=base_model,
            fusion_method=fusion_method
        )

        print(f"âœ… Model created successfully: {model_path}")

        # Generate model card
        print("ğŸ“„ Generating model card...")
        create_model_card(
            model_path=model_path,
            model_name=model_name,
            ontology_file=ontology_file,
            embeddings_file=embeddings_file,
            fusion_method=fusion_method,
            training_config=training_config
        )

        # Generate upload instructions
        print("ğŸ“¤ Generating upload instructions...")
        create_upload_instructions(
            model_path=model_path,
            model_name=model_name
        )

        return model_path

    except Exception as e:
        print(f"âŒ Model creation failed: {e}")
        logger.error(f"Model creation failed: {e}")
        raise


def validate_hf_model(model_path: str, test_queries: Optional[List[str]] = None) -> bool:
    """Test the created model with sample queries."""
    print(f"ğŸ§ª Testing model: {model_path}")

    if test_queries is None:
        test_queries = [
            "heart disease",
            "cardiovascular problems",
            "protein folding",
            "gene expression",
            "genetic mutations"
        ]

    try:
        from sentence_transformers import SentenceTransformer
        from sentence_transformers.util import cos_sim

        # Load model
        model = SentenceTransformer(model_path)
        print(f"ğŸ“ Model dimensions: {model.get_sentence_embedding_dimension()}")

        # Test encoding
        embeddings = model.encode(test_queries)
        print(f"âœ… Encoded {len(test_queries)} queries: {embeddings.shape}")

        # Test similarity
        similarities = cos_sim(embeddings, embeddings)

        print("\nğŸ“Š Sample similarities:")
        for i in range(min(3, len(test_queries))):
            for j in range(i+1, min(3, len(test_queries))):
                sim = similarities[i][j].item()
                print(f"  {test_queries[i][:20]:20} <-> {test_queries[j][:20]:20}: {sim:.3f}")

        print("âœ… Model testing completed successfully!")
        return True

    except Exception as e:
        print(f"âŒ Model testing failed: {e}")
        logger.error(f"Model testing failed: {e}")
        return False


def show_upload_instructions(model_path: str, model_name: str):
    """Show instructions for uploading to HuggingFace Hub."""
    print("\nğŸŒ HuggingFace Hub Upload Instructions")
    print("=" * 50)

    # Show model size
    model_path_obj = Path(model_path)
    total_size = sum(f.stat().st_size for f in model_path_obj.rglob('*') if f.is_file())
    size_mb = total_size / (1024 * 1024)
    print(f"ğŸ“¦ Model size: {size_mb:.1f} MB")

    # Show files
    print("\nğŸ“ Model files:")
    for file in sorted(model_path_obj.rglob("*")):
        if file.is_file():
            file_size = file.stat().st_size / (1024 * 1024)
            rel_path = file.relative_to(model_path_obj)
            print(f"  {str(rel_path):30} {file_size:>6.1f} MB")

    print(f"\nğŸš€ Upload commands:")
    print("# Install huggingface_hub if needed")
    print("pip install huggingface_hub")
    print()
    print("# Login to HuggingFace (one time setup)")
    print("huggingface-cli login")
    print()
    print("# Upload via Python")
    print("python -c \"")
    print("from sentence_transformers import SentenceTransformer")
    print(f"model = SentenceTransformer('{model_path}')")
    print(f"model.push_to_hub('your-username/{model_name}')")
    print("\"")
    print()
    print("# Or upload via CLI")
    print(f"huggingface-cli upload your-username/{model_name} {model_path}")
    print()
    print("ğŸ“– After upload, users can access with:")
    print("from sentence_transformers import SentenceTransformer")
    print(f"model = SentenceTransformer('your-username/{model_name}')")


def end_to_end_workflow(
    owl_file: str,
    model_name: str,
    output_dir: str = "./hf_models",
    embeddings_file: Optional[str] = None,
    base_model: str = "all-MiniLM-L6-v2",
    fusion_method: str = "concat",
    epochs: int = 100,
    skip_training: bool = False,
    skip_testing: bool = False
) -> bool:
    """Run the complete end-to-end workflow."""
    print("ğŸš€ Starting End-to-End Workflow")
    print("=" * 50)
    print(f"ğŸ§¬ OWL file: {owl_file}")
    print(f"ğŸ·ï¸ Model name: {model_name}")
    print(f"ğŸ“ Output directory: {output_dir}")
    print(f"ğŸ¤– Base model: {base_model}")
    print(f"ğŸ”— Fusion method: {fusion_method}")
    print()

    try:
        # Step 1: Train ontology model (if not skipping)
        if embeddings_file is None:
            embeddings_file = f"{Path(owl_file).stem}_embeddings.parquet"

        if not skip_training:
            print("ğŸ“š Step 1: Training ontology model with text features")
            if not train_ontology_with_text(owl_file, embeddings_file, base_model, epochs):
                return False
        else:
            print("ğŸ“š Step 1: Skipping training (using existing embeddings)")

        # Step 2: Validate embeddings
        print("\nğŸ” Step 2: Validating embeddings")
        metadata = validate_embeddings(embeddings_file)
        print(f"   Concepts: {metadata['num_embeddings']:,}")
        print(f"   Text dim: {metadata['text_dim']}")
        print(f"   Structural dim: {metadata['structural_dim']}")

        # Step 3: Create HuggingFace model
        print(f"\nğŸ—ï¸ Step 3: Creating HuggingFace model")

        # For end-to-end workflow, auto-detect unless explicitly specified
        # If user didn't specify base_model in e2e, let create_hf_model auto-detect
        create_base_model = base_model if base_model != 'all-MiniLM-L6-v2' else None

        # Prepare training config for model card
        training_config = {
            'model_type': 'gcn',  # Default - could be enhanced to detect from embeddings
            'epochs': epochs,
            'hidden_dim': 128,  # Default values - could be enhanced
            'out_dim': 64,
            'loss_fn': 'triplet'
        }

        model_path = create_hf_model(
            embeddings_file=embeddings_file,
            model_name=model_name,
            output_dir=output_dir,
            base_model=create_base_model,
            fusion_method=fusion_method,
            validate_first=False,  # Already validated
            ontology_file=owl_file,
            training_config=training_config
        )

        # Step 4: Test model (if not skipping)
        if not skip_testing:
            print(f"\nğŸ§ª Step 4: Testing model")
            if not validate_hf_model(model_path):
                return False
        else:
            print(f"\nğŸ§ª Step 4: Skipping model testing")

        # Step 5: Show upload instructions
        print(f"\nğŸ“¤ Step 5: Upload preparation")
        show_upload_instructions(model_path, model_name)

        print("\n" + "=" * 50)
        print("âœ… End-to-End Workflow Completed Successfully!")
        print(f"ğŸ“¦ Model ready at: {model_path}")
        print("ğŸŒ Ready for HuggingFace Hub upload!")

        return True

    except Exception as e:
        print(f"\nâŒ Workflow failed: {e}")
        logger.error(f"Workflow failed: {e}")
        return False