{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search with on2vec\n",
    "\n",
    "This notebook demonstrates how to use on2vec embeddings for semantic search across ontologies. We'll show how to:\n",
    "\n",
    "1. Find semantically similar concepts within an ontology\n",
    "2. Search for concepts across different ontologies\n",
    "3. Build a semantic search engine using cosine similarity\n",
    "4. Implement query expansion and concept matching\n",
    "5. Visualize semantic neighborhoods\n",
    "\n",
    "## Use Case: Cross-Ontology Concept Discovery\n",
    "Imagine you're working with multiple biomedical ontologies and need to find related concepts across them. Traditional keyword search fails to capture semantic relationships, but embedding-based search can find conceptually similar terms even when they use different vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# on2vec imports\n",
    "from on2vec import (\n",
    "    load_embeddings_as_dataframe,\n",
    "    get_embedding_vector,\n",
    "    train_ontology_embeddings,\n",
    "    embed_ontology_with_model\n",
    ")\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Embeddings for Multiple Ontologies\n",
    "\n",
    "First, let's create embeddings for different ontologies that we'll use for cross-ontology search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 available ontologies: ['edam', 'cvdo', 'duo', 'cro', 'fao']\n",
      "\n",
      "Processing edam ontology: EDAM.owl\n",
      "  ‚úì Using existing model: edam_model.pt\n",
      "  ‚úì Using existing embeddings: edam_embeddings.parquet\n",
      "‚úÖ edam ready for semantic search\n",
      "\n",
      "Processing cvdo ontology: cvdo.owl\n",
      "  ‚úì Using existing model: cvdo_model.pt\n",
      "  ‚úì Using existing embeddings: cvdo_embeddings.parquet\n",
      "‚úÖ cvdo ready for semantic search\n",
      "\n",
      "Processing duo ontology: owl_files/duo.owl\n",
      "  ‚úì Using existing model: duo_model.pt\n",
      "  ‚úì Using existing embeddings: duo_embeddings.parquet\n",
      "‚úÖ duo ready for semantic search\n",
      "\n",
      "Processing cro ontology: owl_files/cro.owl\n",
      "  Training model for cro...\n",
      "  ‚úì Model trained successfully\n",
      "  Generating embeddings for cro...\n",
      "  ‚úì Embeddings generated successfully\n",
      "‚úÖ cro ready for semantic search\n",
      "\n",
      "Processing fao ontology: owl_files/fao.owl\n",
      "  Training model for fao...\n",
      "  ‚úì Model trained successfully\n",
      "  Generating embeddings for fao...\n",
      "  ‚úì Embeddings generated successfully\n",
      "‚úÖ fao ready for semantic search\n",
      "\n",
      "üéØ SEMANTIC SEARCH READY!\n",
      "========================================\n",
      "‚úÖ Successfully loaded 5 ontologies:\n",
      "   ‚Ä¢ edam\n",
      "   ‚Ä¢ cvdo\n",
      "   ‚Ä¢ duo\n",
      "   ‚Ä¢ cro\n",
      "   ‚Ä¢ fao\n",
      "\n",
      "Ready for cross-ontology semantic search!\n"
     ]
    }
   ],
   "source": [
    "# Check if we have existing embeddings, otherwise generate them\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Example ontologies for demonstration - using verified working ontologies\n",
    "ontologies = {\n",
    "    'edam': 'EDAM.owl',  # Bioinformatics (if available)\n",
    "    'cvdo': 'cvdo.owl',  # Cardiovascular disease (if available) \n",
    "    'duo': 'owl_files/duo.owl',  # Data Use Ontology (45 classes, tested working)\n",
    "    'cro': 'owl_files/cro.owl',   # Contributor Role Ontology (105 classes, tested working)\n",
    "    'fao': 'owl_files/fao.owl'    # FAIR* Reviews Ontology (116 classes, tested working)\n",
    "}\n",
    "\n",
    "# Filter to only use ontologies that actually exist\n",
    "available_ontologies = {name: path for name, path in ontologies.items() if os.path.exists(path)}\n",
    "\n",
    "if not available_ontologies:\n",
    "    print(\"‚ùå No ontology files found. Please ensure at least one ontology is available:\")\n",
    "    for name, path in ontologies.items():\n",
    "        print(f\"  ‚Ä¢ {name}: {path}\")\n",
    "    available_ontologies = {}\n",
    "\n",
    "print(f\"Found {len(available_ontologies)} available ontologies: {list(available_ontologies.keys())}\")\n",
    "\n",
    "embedding_files = {}\n",
    "model_files = {}\n",
    "\n",
    "# Generate embeddings for each available ontology\n",
    "for name, owl_file in available_ontologies.items():\n",
    "    model_file = f\"{name}_model.pt\"\n",
    "    embedding_file = f\"{name}_embeddings.parquet\"\n",
    "    \n",
    "    print(f\"\\nProcessing {name} ontology: {owl_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Train model if it doesn't exist\n",
    "        if not os.path.exists(model_file):\n",
    "            print(f\"  Training model for {name}...\")\n",
    "            result = train_ontology_embeddings(\n",
    "                owl_file=owl_file,\n",
    "                model_output=model_file,\n",
    "                model_type=\"gcn\",\n",
    "                hidden_dim=128,\n",
    "                out_dim=64,\n",
    "                epochs=50,  # Reduced for demo\n",
    "                learning_rate=0.01\n",
    "            )\n",
    "            print(f\"  ‚úì Model trained successfully\")\n",
    "        else:\n",
    "            print(f\"  ‚úì Using existing model: {model_file}\")\n",
    "        \n",
    "        # Generate embeddings if they don't exist\n",
    "        if not os.path.exists(embedding_file):\n",
    "            print(f\"  Generating embeddings for {name}...\")\n",
    "            embeddings = embed_ontology_with_model(\n",
    "                model_path=model_file,\n",
    "                owl_file=owl_file,\n",
    "                output_file=embedding_file\n",
    "            )\n",
    "            print(f\"  ‚úì Embeddings generated successfully\")\n",
    "        else:\n",
    "            print(f\"  ‚úì Using existing embeddings: {embedding_file}\")\n",
    "        \n",
    "        embedding_files[name] = embedding_file\n",
    "        model_files[name] = model_file\n",
    "        print(f\"‚úÖ {name} ready for semantic search\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {name}: {str(e)[:100]}...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nüéØ SEMANTIC SEARCH READY!\")\n",
    "print(\"=\" * 40)\n",
    "if len(embedding_files) == 0:\n",
    "    print(\"‚ùå No ontologies processed successfully.\")\n",
    "    print(\"Please check that the ontology files exist and are valid.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Successfully loaded {len(embedding_files)} ontologies:\")\n",
    "    for name in embedding_files.keys():\n",
    "        print(f\"   ‚Ä¢ {name}\")\n",
    "    print(f\"\\nReady for cross-ontology semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Semantic Search Engine\n",
    "\n",
    "Let's create a semantic search engine that can find similar concepts across ontologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading edam embeddings...\n",
      "  ‚úì Loaded 3,511 concepts from edam\n",
      "Loading cvdo embeddings...\n",
      "  ‚úì Loaded 720 concepts from cvdo\n",
      "Loading duo embeddings...\n",
      "  ‚úì Loaded 45 concepts from duo\n",
      "Loading cro embeddings...\n",
      "  ‚úì Loaded 105 concepts from cro\n",
      "Loading fao embeddings...\n",
      "  ‚úì Loaded 116 concepts from fao\n",
      "\n",
      "Search engine ready with 4,496 unique concept names!\n"
     ]
    }
   ],
   "source": [
    "class SemanticSearchEngine:\n",
    "    def __init__(self, embedding_files):\n",
    "        \"\"\"Initialize search engine with multiple ontology embeddings.\"\"\"\n",
    "        self.ontologies = {}\n",
    "        self.embeddings = {}\n",
    "        self.concept_index = {}\n",
    "        \n",
    "        # Load all embeddings\n",
    "        for name, file_path in embedding_files.items():\n",
    "            print(f\"Loading {name} embeddings...\")\n",
    "            df, metadata = load_embeddings_as_dataframe(file_path, return_metadata=True)\n",
    "            \n",
    "            # Store embeddings as numpy array\n",
    "            embeddings_matrix = np.stack(df['embedding'].to_numpy())\n",
    "            node_ids = df['node_id'].to_numpy()\n",
    "            \n",
    "            self.ontologies[name] = {\n",
    "                'df': df,\n",
    "                'metadata': metadata,\n",
    "                'embeddings': embeddings_matrix,\n",
    "                'node_ids': node_ids,\n",
    "                'id_to_idx': {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
    "            }\n",
    "            \n",
    "            # Build concept index for text-based lookup\n",
    "            for idx, node_id in enumerate(node_ids):\n",
    "                # Extract concept name from IRI\n",
    "                concept_name = node_id.split('/')[-1].split('#')[-1].replace('_', ' ').lower()\n",
    "                if concept_name not in self.concept_index:\n",
    "                    self.concept_index[concept_name] = []\n",
    "                self.concept_index[concept_name].append({\n",
    "                    'ontology': name,\n",
    "                    'node_id': node_id,\n",
    "                    'idx': idx\n",
    "                })\n",
    "            \n",
    "            print(f\"  ‚úì Loaded {len(node_ids):,} concepts from {name}\")\n",
    "        \n",
    "        print(f\"\\nSearch engine ready with {len(self.concept_index):,} unique concept names!\")\n",
    "    \n",
    "    def find_concept(self, query):\n",
    "        \"\"\"Find concepts matching a text query.\"\"\"\n",
    "        query = query.lower()\n",
    "        matches = []\n",
    "        \n",
    "        for concept_name, concept_info in self.concept_index.items():\n",
    "            if query in concept_name:\n",
    "                matches.extend(concept_info)\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def get_embedding(self, ontology, node_id):\n",
    "        \"\"\"Get embedding vector for a specific concept.\"\"\"\n",
    "        if ontology not in self.ontologies:\n",
    "            return None\n",
    "        \n",
    "        ont_data = self.ontologies[ontology]\n",
    "        if node_id not in ont_data['id_to_idx']:\n",
    "            return None\n",
    "        \n",
    "        idx = ont_data['id_to_idx'][node_id]\n",
    "        return ont_data['embeddings'][idx]\n",
    "    \n",
    "    def semantic_search(self, query_concept, ontology=None, top_k=10, min_similarity=0.5):\n",
    "        \"\"\"Find semantically similar concepts across ontologies.\"\"\"\n",
    "        # Get query embedding\n",
    "        if ontology and ontology in self.ontologies:\n",
    "            query_embedding = self.get_embedding(ontology, query_concept)\n",
    "            if query_embedding is None:\n",
    "                return []\n",
    "        else:\n",
    "            # Try to find the concept in any ontology\n",
    "            query_embedding = None\n",
    "            for ont_name in self.ontologies:\n",
    "                query_embedding = self.get_embedding(ont_name, query_concept)\n",
    "                if query_embedding is not None:\n",
    "                    ontology = ont_name\n",
    "                    break\n",
    "            \n",
    "            if query_embedding is None:\n",
    "                return []\n",
    "        \n",
    "        # Search across all ontologies\n",
    "        results = []\n",
    "        \n",
    "        for ont_name, ont_data in self.ontologies.items():\n",
    "            # Calculate similarities\n",
    "            similarities = cosine_similarity(\n",
    "                query_embedding.reshape(1, -1), \n",
    "                ont_data['embeddings']\n",
    "            )[0]\n",
    "            \n",
    "            # Find top similar concepts\n",
    "            for idx, similarity in enumerate(similarities):\n",
    "                if similarity >= min_similarity:\n",
    "                    node_id = ont_data['node_ids'][idx]\n",
    "                    concept_name = node_id.split('/')[-1].split('#')[-1].replace('_', ' ')\n",
    "                    \n",
    "                    results.append({\n",
    "                        'ontology': ont_name,\n",
    "                        'node_id': node_id,\n",
    "                        'concept_name': concept_name,\n",
    "                        'similarity': float(similarity),\n",
    "                        'is_query': ont_name == ontology and node_id == query_concept\n",
    "                    })\n",
    "        \n",
    "        # Sort by similarity and return top_k\n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:top_k]\n",
    "    \n",
    "    def cross_ontology_search(self, query_text, top_k=20):\n",
    "        \"\"\"Search for concepts across ontologies using text query.\"\"\"\n",
    "        # First find concepts matching the text\n",
    "        concept_matches = self.find_concept(query_text)\n",
    "        \n",
    "        if not concept_matches:\n",
    "            print(f\"No concepts found matching '{query_text}'\")\n",
    "            return []\n",
    "        \n",
    "        # Use the first match as query concept\n",
    "        query_match = concept_matches[0]\n",
    "        print(f\"Using query concept: {query_match['concept_name']} from {query_match['ontology']}\")\n",
    "        \n",
    "        # Perform semantic search\n",
    "        return self.semantic_search(\n",
    "            query_match['node_id'], \n",
    "            query_match['ontology'], \n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "# Initialize search engine\n",
    "if embedding_files:\n",
    "    search_engine = SemanticSearchEngine(embedding_files)\n",
    "else:\n",
    "    print(\"No embedding files available for search engine initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Semantic Search Examples\n",
    "\n",
    "Let's demonstrate the semantic search capabilities with real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SEMANTIC SEARCH EXAMPLE 1: Protein-related concepts\n",
      "============================================================\n",
      "No concepts found matching 'file'\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Search for protein-related concepts\n",
    "if 'search_engine' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SEMANTIC SEARCH EXAMPLE 1: Protein-related concepts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = search_engine.cross_ontology_search(\"protein\", top_k=15)\n",
    "    \n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(f\"\\nFound {len(results)} similar concepts:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:10]):\n",
    "            marker = \"üéØ\" if result['is_query'] else \"üîç\"\n",
    "            print(f\"{marker} {i+1:2d}. {result['concept_name']:40} | {result['ontology']:8} | {result['similarity']:.3f}\")\n",
    "        \n",
    "        # Show distribution by ontology\n",
    "        print(\"\\nDistribution by ontology:\")\n",
    "        ont_counts = results_df['ontology'].value_counts()\n",
    "        for ont, count in ont_counts.items():\n",
    "            print(f\"  {ont}: {count} concepts\")\n",
    "else:\n",
    "    print(\"Search engine not available - please run previous cells first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Search for disease-related concepts\n",
    "if 'search_engine' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SEMANTIC SEARCH EXAMPLE 2: Disease-related concepts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = search_engine.cross_ontology_search(\"disease\", top_k=15)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} similar concepts:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:10]):\n",
    "            marker = \"üéØ\" if result['is_query'] else \"üîç\"\n",
    "            print(f\"{marker} {i+1:2d}. {result['concept_name']:40} | {result['ontology']:8} | {result['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SEMANTIC SEARCH EXAMPLE 3: Data format concepts\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'concept_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEMANTIC SEARCH EXAMPLE 3: Data format concepts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msearch_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_ontology_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m similar concepts:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 122\u001b[0m, in \u001b[0;36mSemanticSearchEngine.cross_ontology_search\u001b[0;34m(self, query_text, top_k)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Use the first match as query concept\u001b[39;00m\n\u001b[1;32m    121\u001b[0m query_match \u001b[38;5;241m=\u001b[39m concept_matches[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing query concept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mquery_match\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconcept_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_match[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124montology\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Perform semantic search\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_search(\n\u001b[1;32m    126\u001b[0m     query_match[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    127\u001b[0m     query_match[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124montology\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m    128\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k\n\u001b[1;32m    129\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'concept_name'"
     ]
    }
   ],
   "source": [
    "# Example 3: Search for data/format concepts\n",
    "if 'search_engine' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SEMANTIC SEARCH EXAMPLE 3: Data format concepts\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = search_engine.cross_ontology_search(\"format\", top_k=15)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nFound {len(results)} similar concepts:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:10]):\n",
    "            marker = \"üéØ\" if result['is_query'] else \"üîç\"\n",
    "            print(f\"{marker} {i+1:2d}. {result['concept_name']:40} | {result['ontology']:8} | {result['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Semantic Neighborhoods\n",
    "\n",
    "Let's create interactive visualizations to explore semantic neighborhoods around concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_semantic_neighborhood(search_engine, query_text, top_k=50):\n",
    "    \"\"\"Create an interactive visualization of semantic neighborhood.\"\"\"\n",
    "    results = search_engine.cross_ontology_search(query_text, top_k=top_k)\n",
    "    \n",
    "    if len(results) < 5:\n",
    "        print(f\"Not enough results to visualize (found {len(results)})\")\n",
    "        return None\n",
    "    \n",
    "    # Collect embeddings for visualization\n",
    "    embeddings_viz = []\n",
    "    labels = []\n",
    "    ontologies = []\n",
    "    similarities = []\n",
    "    \n",
    "    for result in results:\n",
    "        embedding = search_engine.get_embedding(result['ontology'], result['node_id'])\n",
    "        if embedding is not None:\n",
    "            embeddings_viz.append(embedding)\n",
    "            labels.append(result['concept_name'])\n",
    "            ontologies.append(result['ontology'])\n",
    "            similarities.append(result['similarity'])\n",
    "    \n",
    "    if len(embeddings_viz) < 5:\n",
    "        print(\"Not enough valid embeddings for visualization\")\n",
    "        return None\n",
    "    \n",
    "    embeddings_matrix = np.array(embeddings_viz)\n",
    "    \n",
    "    # Reduce dimensionality for visualization\n",
    "    print(f\"Reducing dimensionality from {embeddings_matrix.shape[1]}D to 2D...\")\n",
    "    \n",
    "    # Use UMAP for better preservation of local structure\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, min_dist=0.1, n_neighbors=15)\n",
    "    embeddings_2d = reducer.fit_transform(embeddings_matrix)\n",
    "    \n",
    "    # Create interactive plot\n",
    "    colors = px.colors.qualitative.Set3\n",
    "    ontology_colors = {ont: colors[i % len(colors)] for i, ont in enumerate(set(ontologies))}\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add points for each ontology\n",
    "    for ont in set(ontologies):\n",
    "        mask = [o == ont for o in ontologies]\n",
    "        x_vals = embeddings_2d[mask, 0]\n",
    "        y_vals = embeddings_2d[mask, 1]\n",
    "        \n",
    "        ont_labels = [labels[i] for i, m in enumerate(mask) if m]\n",
    "        ont_sims = [similarities[i] for i, m in enumerate(mask) if m]\n",
    "        \n",
    "        # Size points by similarity\n",
    "        sizes = [max(8, sim * 20) for sim in ont_sims]\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_vals,\n",
    "            y=y_vals,\n",
    "            mode='markers+text',\n",
    "            name=f'{ont} ({sum(mask)} concepts)',\n",
    "            text=ont_labels,\n",
    "            textposition='top center',\n",
    "            marker=dict(\n",
    "                size=sizes,\n",
    "                color=ontology_colors[ont],\n",
    "                opacity=0.7,\n",
    "                line=dict(width=1, color='white')\n",
    "            ),\n",
    "            customdata=ont_sims,\n",
    "            hovertemplate='<b>%{text}</b><br>' +\n",
    "                         'Ontology: ' + ont + '<br>' +\n",
    "                         'Similarity: %{customdata:.3f}<br>' +\n",
    "                         '<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    # Highlight query concept\n",
    "    query_idx = 0  # First result is always the query\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[embeddings_2d[query_idx, 0]],\n",
    "        y=[embeddings_2d[query_idx, 1]],\n",
    "        mode='markers',\n",
    "        name='Query Concept',\n",
    "        marker=dict(\n",
    "            size=25,\n",
    "            color='red',\n",
    "            symbol='star',\n",
    "            line=dict(width=2, color='darkred')\n",
    "        ),\n",
    "        hovertemplate=f'<b>QUERY: {labels[query_idx]}</b><br>' +\n",
    "                     f'Ontology: {ontologies[query_idx]}<br>' +\n",
    "                     '<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Semantic Neighborhood: \"{query_text}\"<br><sub>Concepts colored by ontology, sized by similarity</sub>',\n",
    "        xaxis_title='UMAP Dimension 1',\n",
    "        yaxis_title='UMAP Dimension 2',\n",
    "        width=1000,\n",
    "        height=700,\n",
    "        hovermode='closest',\n",
    "        legend=dict(x=1.02, y=1)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create visualizations for different queries\n",
    "if 'search_engine' in locals():\n",
    "    queries = [\"protein\", \"disease\", \"format\"]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nCreating visualization for '{query}'...\")\n",
    "        fig = visualize_semantic_neighborhood(search_engine, query, top_k=30)\n",
    "        if fig:\n",
    "            fig.show()\n",
    "        else:\n",
    "            print(f\"Could not create visualization for '{query}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Advanced Search Features\n",
    "\n",
    "Let's implement more advanced search capabilities like query expansion and multi-concept queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSemanticSearch:\n",
    "    def __init__(self, search_engine):\n",
    "        self.search_engine = search_engine\n",
    "    \n",
    "    def query_expansion(self, initial_query, expansion_threshold=0.8, max_expansions=5):\n",
    "        \"\"\"Expand query by finding highly similar concepts.\"\"\"\n",
    "        # Get initial results\n",
    "        initial_results = self.search_engine.cross_ontology_search(initial_query, top_k=20)\n",
    "        \n",
    "        if not initial_results:\n",
    "            return []\n",
    "        \n",
    "        # Find concepts with high similarity to expand the query\n",
    "        expansion_concepts = []\n",
    "        for result in initial_results[:max_expansions+1]:  # +1 for query itself\n",
    "            if result['similarity'] >= expansion_threshold:\n",
    "                expansion_concepts.append(result)\n",
    "        \n",
    "        print(f\"Expanding query '{initial_query}' with {len(expansion_concepts)-1} similar concepts:\")\n",
    "        for concept in expansion_concepts[1:]:  # Skip query itself\n",
    "            print(f\"  + {concept['concept_name']} (sim: {concept['similarity']:.3f})\")\n",
    "        \n",
    "        # Collect all results from expanded queries\n",
    "        all_results = []\n",
    "        seen_concepts = set()\n",
    "        \n",
    "        for concept in expansion_concepts:\n",
    "            results = self.search_engine.semantic_search(\n",
    "                concept['node_id'], \n",
    "                concept['ontology'], \n",
    "                top_k=15,\n",
    "                min_similarity=0.4\n",
    "            )\n",
    "            \n",
    "            for result in results:\n",
    "                concept_key = (result['ontology'], result['node_id'])\n",
    "                if concept_key not in seen_concepts:\n",
    "                    all_results.append(result)\n",
    "                    seen_concepts.add(concept_key)\n",
    "        \n",
    "        # Sort by similarity and return\n",
    "        all_results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return all_results\n",
    "    \n",
    "    def multi_concept_search(self, concept_queries, combination_method='average'):\n",
    "        \"\"\"Search using multiple concepts combined.\"\"\"\n",
    "        concept_embeddings = []\n",
    "        concept_info = []\n",
    "        \n",
    "        # Get embeddings for each query concept\n",
    "        for query in concept_queries:\n",
    "            matches = self.search_engine.find_concept(query)\n",
    "            if matches:\n",
    "                match = matches[0]\n",
    "                embedding = self.search_engine.get_embedding(match['ontology'], match['node_id'])\n",
    "                if embedding is not None:\n",
    "                    concept_embeddings.append(embedding)\n",
    "                    concept_info.append(match)\n",
    "        \n",
    "        if len(concept_embeddings) < 2:\n",
    "            print(\"Need at least 2 valid concepts for multi-concept search\")\n",
    "            return []\n",
    "        \n",
    "        # Combine embeddings\n",
    "        if combination_method == 'average':\n",
    "            combined_embedding = np.mean(concept_embeddings, axis=0)\n",
    "        elif combination_method == 'max':\n",
    "            combined_embedding = np.max(concept_embeddings, axis=0)\n",
    "        else:\n",
    "            # Weighted average (equal weights for now)\n",
    "            weights = np.ones(len(concept_embeddings)) / len(concept_embeddings)\n",
    "            combined_embedding = np.average(concept_embeddings, axis=0, weights=weights)\n",
    "        \n",
    "        print(f\"Searching with combined concept: {' + '.join(concept_queries)}\")\n",
    "        print(f\"Combination method: {combination_method}\")\n",
    "        \n",
    "        # Search using combined embedding\n",
    "        results = []\n",
    "        for ont_name, ont_data in self.search_engine.ontologies.items():\n",
    "            similarities = cosine_similarity(\n",
    "                combined_embedding.reshape(1, -1),\n",
    "                ont_data['embeddings']\n",
    "            )[0]\n",
    "            \n",
    "            for idx, similarity in enumerate(similarities):\n",
    "                if similarity >= 0.3:  # Lower threshold for combined queries\n",
    "                    node_id = ont_data['node_ids'][idx]\n",
    "                    concept_name = node_id.split('/')[-1].split('#')[-1].replace('_', ' ')\n",
    "                    \n",
    "                    results.append({\n",
    "                        'ontology': ont_name,\n",
    "                        'node_id': node_id,\n",
    "                        'concept_name': concept_name,\n",
    "                        'similarity': float(similarity),\n",
    "                        'is_multi_concept': True\n",
    "                    })\n",
    "        \n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        return results[:20]\n",
    "    \n",
    "    def semantic_clustering(self, concept_list, n_clusters=5):\n",
    "        \"\"\"Group concepts into semantic clusters.\"\"\"\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        embeddings = []\n",
    "        valid_concepts = []\n",
    "        \n",
    "        for concept in concept_list:\n",
    "            matches = self.search_engine.find_concept(concept)\n",
    "            if matches:\n",
    "                match = matches[0]\n",
    "                embedding = self.search_engine.get_embedding(match['ontology'], match['node_id'])\n",
    "                if embedding is not None:\n",
    "                    embeddings.append(embedding)\n",
    "                    valid_concepts.append({\n",
    "                        'query': concept,\n",
    "                        'concept_name': match['node_id'].split('/')[-1].split('#')[-1].replace('_', ' '),\n",
    "                        'ontology': match['ontology']\n",
    "                    })\n",
    "        \n",
    "        if len(embeddings) < n_clusters:\n",
    "            print(f\"Need at least {n_clusters} concepts for clustering\")\n",
    "            return {}\n",
    "        \n",
    "        # Perform clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Group concepts by cluster\n",
    "        clustered_concepts = {}\n",
    "        for i, cluster_id in enumerate(clusters):\n",
    "            if cluster_id not in clustered_concepts:\n",
    "                clustered_concepts[cluster_id] = []\n",
    "            clustered_concepts[cluster_id].append(valid_concepts[i])\n",
    "        \n",
    "        return clustered_concepts\n",
    "\n",
    "# Initialize advanced search\n",
    "if 'search_engine' in locals():\n",
    "    advanced_search = AdvancedSemanticSearch(search_engine)\n",
    "    print(\"Advanced semantic search features ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query Expansion\n",
    "if 'advanced_search' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUERY EXPANSION EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    expanded_results = advanced_search.query_expansion(\"protein\", expansion_threshold=0.7)\n",
    "    \n",
    "    if expanded_results:\n",
    "        print(f\"\\nExpanded search results ({len(expanded_results)} concepts):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(expanded_results[:15]):\n",
    "            print(f\"{i+1:2d}. {result['concept_name']:40} | {result['ontology']:8} | {result['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Multi-concept Search\n",
    "if 'advanced_search' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MULTI-CONCEPT SEARCH EXAMPLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Search for concepts that combine multiple ideas\n",
    "    multi_results = advanced_search.multi_concept_search([\"protein\", \"structure\", \"data\"])\n",
    "    \n",
    "    if multi_results:\n",
    "        print(f\"\\nMulti-concept search results ({len(multi_results)} concepts):\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(multi_results[:15]):\n",
    "            print(f\"{i+1:2d}. {result['concept_name']:40} | {result['ontology']:8} | {result['similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Analysis\n",
    "\n",
    "Let's analyze the performance and coverage of our semantic search system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_search_performance(search_engine):\n",
    "    \"\"\"Analyze the performance and coverage of the search engine.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SEMANTIC SEARCH PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Coverage statistics\n",
    "    total_concepts = sum(len(ont_data['node_ids']) for ont_data in search_engine.ontologies.values())\n",
    "    unique_concept_names = len(search_engine.concept_index)\n",
    "    \n",
    "    print(f\"\\nüìä Coverage Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total concepts across all ontologies: {total_concepts:,}\")\n",
    "    print(f\"  ‚Ä¢ Unique concept names (searchable): {unique_concept_names:,}\")\n",
    "    print(f\"  ‚Ä¢ Coverage ratio: {unique_concept_names/total_concepts:.1%}\")\n",
    "    \n",
    "    # Per-ontology statistics\n",
    "    print(f\"\\nüóÇÔ∏è Per-ontology breakdown:\")\n",
    "    for ont_name, ont_data in search_engine.ontologies.items():\n",
    "        n_concepts = len(ont_data['node_ids'])\n",
    "        embedding_dim = ont_data['embeddings'].shape[1]\n",
    "        print(f\"  ‚Ä¢ {ont_name:10}: {n_concepts:,} concepts, {embedding_dim}D embeddings\")\n",
    "    \n",
    "    # Test search performance with sample queries\n",
    "    test_queries = [\"protein\", \"disease\", \"cell\", \"gene\", \"data\", \"format\", \"structure\"]\n",
    "    \n",
    "    print(f\"\\nüîç Search performance test:\")\n",
    "    performance_stats = []\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for query in test_queries:\n",
    "        start_time = time.time()\n",
    "        results = search_engine.cross_ontology_search(query, top_k=20)\n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        n_results = len(results)\n",
    "        n_ontologies = len(set(r['ontology'] for r in results)) if results else 0\n",
    "        avg_similarity = np.mean([r['similarity'] for r in results]) if results else 0\n",
    "        \n",
    "        performance_stats.append({\n",
    "            'query': query,\n",
    "            'n_results': n_results,\n",
    "            'n_ontologies': n_ontologies,\n",
    "            'avg_similarity': avg_similarity,\n",
    "            'search_time_ms': search_time * 1000\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚Ä¢ '{query:8}': {n_results:2d} results, {n_ontologies} ontologies, \"\n",
    "              f\"avg_sim={avg_similarity:.3f}, {search_time*1000:.1f}ms\")\n",
    "    \n",
    "    # Create performance visualization\n",
    "    perf_df = pd.DataFrame(performance_stats)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Number of results by query\n",
    "    ax1.bar(perf_df['query'], perf_df['n_results'], color='skyblue')\n",
    "    ax1.set_title('Number of Results by Query')\n",
    "    ax1.set_ylabel('Number of Results')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average similarity by query\n",
    "    ax2.bar(perf_df['query'], perf_df['avg_similarity'], color='lightgreen')\n",
    "    ax2.set_title('Average Similarity by Query')\n",
    "    ax2.set_ylabel('Average Similarity')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Search time by query\n",
    "    ax3.bar(perf_df['query'], perf_df['search_time_ms'], color='coral')\n",
    "    ax3.set_title('Search Time by Query (ms)')\n",
    "    ax3.set_ylabel('Time (milliseconds)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Ontology coverage by query\n",
    "    ax4.bar(perf_df['query'], perf_df['n_ontologies'], color='gold')\n",
    "    ax4.set_title('Ontologies Covered by Query')\n",
    "    ax4.set_ylabel('Number of Ontologies')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return performance_stats\n",
    "\n",
    "# Run performance analysis\n",
    "if 'search_engine' in locals():\n",
    "    perf_stats = analyze_search_performance(search_engine)\n",
    "else:\n",
    "    print(\"Search engine not available for performance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how on2vec embeddings enable powerful semantic search across ontologies:\n",
    "\n",
    "### ‚úÖ Key Capabilities Demonstrated:\n",
    "\n",
    "1. **Cross-Ontology Search**: Find related concepts across different domain ontologies\n",
    "2. **Semantic Similarity**: Discover conceptually similar terms even with different vocabulary\n",
    "3. **Interactive Visualization**: Explore semantic neighborhoods in 2D space\n",
    "4. **Query Expansion**: Automatically expand searches with similar concepts\n",
    "5. **Multi-Concept Queries**: Combine multiple concepts for complex searches\n",
    "6. **Performance Analysis**: Measure search quality and speed\n",
    "\n",
    "### üöÄ Real-World Applications:\n",
    "\n",
    "- **Literature Search**: Find relevant papers across domains using semantic concept matching\n",
    "- **Data Discovery**: Locate datasets and resources using conceptual similarity\n",
    "- **Knowledge Integration**: Bridge different domain vocabularies automatically\n",
    "- **Recommendation Systems**: Suggest related concepts, tools, or methods\n",
    "- **Ontology Harmonization**: Identify equivalent concepts across ontologies\n",
    "\n",
    "### üîÑ Next Steps:\n",
    "\n",
    "1. **Scale to Larger Ontologies**: Test with full-size ontologies (GO, ChEBI, etc.)\n",
    "2. **Domain-Specific Tuning**: Fine-tune embeddings for specific application domains\n",
    "3. **Multi-Modal Search**: Combine text, structure, and metadata for richer search\n",
    "4. **Real-Time Applications**: Deploy as web service with caching and optimization\n",
    "5. **User Interface**: Build interactive web interface for non-technical users\n",
    "\n",
    "The semantic search capabilities shown here demonstrate the practical value of on2vec embeddings for real-world knowledge discovery and integration tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
